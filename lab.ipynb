{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7e43ce9-c243-4046-8695-9c10f75f6a91",
   "metadata": {},
   "source": [
    "### Loading the Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc525e8-3db2-4182-93b0-c401863af600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('azure.env',override = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d3650-546a-4b4f-99b1-7151fd429e3f",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e2f82-97c8-48f6-98a8-abe25c0e30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings,AzureChatOpenAI\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.schema import Document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import PostgresChatMessageHistory\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "from azure.ai.documentintelligence.models import DocumentAnalysisFeature\n",
    "from langchain_community.document_loaders.doc_intelligence import AzureAIDocumentIntelligenceLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd5389-4c40-463a-893a-c331525e7e33",
   "metadata": {},
   "source": [
    "### Uploading the document through Azure Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b49c8-fa1e-41fa-ad4b-f71e8b1b224d",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "api_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "# print(f\"API Key: {api_key}, Type: {type(api_key)}\")\n",
    "# print(f\"API Endpoint: {api_endpoint}, Type: {type(api_endpoint)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771883b-0548-4db9-86b6-2ae107dce3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AzureAIDocumentIntelligenceLoader(file_path=r'C:\\Users\\nag\\Documents\\Microsoft\\Customer data\\Infy_Helix_Data\\multi_page_table.pdf', \n",
    "                                           api_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\"), \n",
    "                                           api_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\"),\n",
    "                                           api_model=\"prebuilt-layout\",\n",
    "                                           api_version=\"2024-02-29-preview\",\n",
    "                                           mode='markdown',\n",
    "                                           analysis_features = [DocumentAnalysisFeature.OCR_HIGH_RESOLUTION])\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87844736-64d5-496e-8205-3408e20a7fb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(docs[-1].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b63875-247a-4313-be9a-f33f5b9ff1fc",
   "metadata": {},
   "source": [
    "### Split the document into chunks base on markdown headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0347d-42c0-41e4-96ae-ce0806bb4b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),  \n",
    "    (\"#######\", \"Header 7\"), \n",
    "    (\"########\", \"Header 8\")\n",
    "]\n",
    "text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "\n",
    "docs_string = docs[0].page_content\n",
    "docs_result = text_splitter.split_text(docs_string)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(docs_result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5704e50c-c7b5-44c7-865d-b9e4d658ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_result[6].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc962d67-2cbb-4962-9d87-5a74ebd98e08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T13:13:58.210953Z",
     "iopub.status.busy": "2024-07-19T13:13:58.209399Z",
     "iopub.status.idle": "2024-07-19T13:13:58.219798Z",
     "shell.execute_reply": "2024-07-19T13:13:58.216682Z",
     "shell.execute_reply.started": "2024-07-19T13:13:58.210953Z"
    }
   },
   "source": [
    "### Character Splitter to Split based on Chunk Size as well as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29494f5a-f48c-4ed2-8773-903507001cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "from langchain_text_splitters.base import Language, TextSplitter\n",
    "\n",
    "class CustomCharacterTextSplitter(TextSplitter):\n",
    "    \"\"\"Splitting text that looks at characters.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, separator: str = \"\\n\\n\", is_separator_regex: bool = False, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new TextSplitter.\"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self._separator = separator\n",
    "        self._is_separator_regex = is_separator_regex\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks.\"\"\"\n",
    "        # First we naively split the large input into a bunch of smaller ones.\n",
    "        separator = (\n",
    "            self._separator if self._is_separator_regex else re.escape(self._separator)\n",
    "        )\n",
    "        splits = re.split(separator, text, flags=re.DOTALL) \n",
    "        splits = [part for part in splits if part.strip()]\n",
    "        return splits\n",
    "\n",
    "text_splitter = CustomCharacterTextSplitter(separator=r'(<figure>.*?</figure>)', is_separator_regex=True)\n",
    "child_docs  = text_splitter.split_documents(docs_result)\n",
    "print(\"Length of splits: \" + str(len(child_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d8c0da-fd73-4bd7-8ea2-fd47194d24ea",
   "metadata": {},
   "source": [
    "### Load the LangChain OpenAI Embedding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad5fdb5-7131-446c-bb87-83cc9190f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoai_embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    openai_api_version=\"2024-03-01-preview\",\n",
    "    azure_endpoint =os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0098fec1-de27-4941-9d5f-226abf50abb2",
   "metadata": {},
   "source": [
    "### Create the Azure AI Search Index Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05359122-9bc3-43fc-9d78-515f6a2de8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "from azure.search.documents.indexes.models import (\n",
    "    ScoringProfile,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    TextWeights\n",
    ")\n",
    "embedding_function = aoai_embeddings.embed_query\n",
    "fields = [\n",
    "    SimpleField(\n",
    "        name=\"id\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        key=True,\n",
    "        filterable=True,\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"content\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    SearchField(\n",
    "        name=\"content_vector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        vector_search_dimensions=len(embedding_function(\"Text\")),\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "    SearchableField(\n",
    "        name=\"metadata\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=False,\n",
    "    ),\n",
    "    # Additional field to store the title\n",
    "    SearchableField(\n",
    "        name=\"header\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        searchable=True,\n",
    "    ),\n",
    "    # Additional field for filtering on document source\n",
    "    SimpleField(\n",
    "        name=\"image\",\n",
    "        type=SearchFieldDataType.String,\n",
    "        filterable=False,\n",
    "        searchable=False,\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76eebb-a56d-4b2d-a729-f1cd3c7eb195",
   "metadata": {},
   "source": [
    "### Create the AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7b3eff-2ece-48cd-b370-0dc750bf738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name: str = \"langchain-vector-demo-custom3\"\n",
    "\n",
    "vector_store_multi_modal: AzureSearch = AzureSearch(\n",
    "    azure_search_endpoint=os.environ[\"AZURE_SEARCH_ENDPOINT\"],\n",
    "    azure_search_key=os.environ[\"AZURE_SEARCH_KEY\"],\n",
    "    index_name=index_name,\n",
    "    embedding_function=embedding_function,\n",
    "    fields=fields,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8d686-f670-4378-8205-d6b8975c04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "def find_figure_indices(text):\n",
    "    pattern = r'!\\[\\]\\(figures/(\\d+)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    indices = [int(match) for match in matches]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10840850-da0e-4f35-b702-1b1c4dfd056a",
   "metadata": {},
   "source": [
    "### Ingest the chunks into Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6974b2-304c-4ed4-9c81-6e7a3bf8829a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image_metadata = docs[-1].metadata['images']\n",
    "lst_docs = []\n",
    "for doc in child_docs:\n",
    "    figure_indices = find_figure_indices(doc.page_content)\n",
    "    if figure_indices:\n",
    "        for figure_indice in figure_indices:\n",
    "            image = image_metadata[figure_indice]\n",
    "            doc_result = Document(page_content = doc.page_content, metadata={\"header\": json.dumps(doc.metadata), \"source\": \"multi_page_table.pdf\", \"image\": image})\n",
    "            lst_docs.append(doc_result)\n",
    "    else:\n",
    "        doc_result = Document(page_content = doc.page_content, metadata={\"header\": json.dumps(doc.metadata), \"source\": \"multi_page_table.pdf\", \"image\": None})\n",
    "        lst_docs.append(doc_result)\n",
    "vector_store_multi_modal.add_documents(documents=lst_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bff248-b474-4a6d-9453-44bfd32236e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = await FAISS.afrom_documents(documents=child_docs, embedding=aoai_embeddings)\n",
    "retriever_base = index.as_retriever(search_type=\"similarity\",search_kwargs = {\"k\" : 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8643ba3-ae51-4357-a2e0-cfc1a46e813f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Lets do the RAG Now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc6216c-d26f-4845-8e8a-ccf1d34fb0ae",
   "metadata": {},
   "source": [
    "### Load the AOAI Chat Class from LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f0c4a-0fe7-4d2c-8b8b-f1274f5ceba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(api_key = os.environ[\"AZURE_OPENAI_API_KEY\"],  \n",
    "                      api_version = \"2024-06-01\",\n",
    "                      azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "                      model= os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                      streaming=True)\n",
    "llm([HumanMessage(\"Hi\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73033ec9-2458-4002-bb2b-c080cb7cc703",
   "metadata": {},
   "source": [
    "### Multi Modal RAG (Ingestion Side Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e4c2f9-0727-49bd-95af-c13150223e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "def format_docs(docs):\n",
    "    to_return =  \"\\n\\n\".join(str(doc.metadata) + \"\\n\" + doc.page_content for doc in docs)\n",
    "    return to_return\n",
    "    \n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "rag_chain_with_source = RunnableMap(\n",
    "    {\"documents\": retriever_base, \"question\": RunnablePassthrough()}\n",
    ") | {\n",
    "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
    "    \"answer\": rag_chain_from_docs,\n",
    "}\n",
    "rag_chain_with_source.invoke(\"Does Quality consultant has Controlling field office write permission?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61eea339-2b2f-4332-add7-cdf739f02f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source.invoke(\"Does Quality Consultant has any write permission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882efb51-8166-4e96-a998-1ec4db0d3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain_with_source.invoke(\"what permissions does Implementation Manager has in terms of write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4434cf-daa1-4920-8e1f-c038b29dfb17",
   "metadata": {},
   "source": [
    "### Multi Modal RAG (Both Ingestion Side + Calling Side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793d1810-1bd7-4cbc-bc92-b623138f98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "def get_image_text(docs):\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        if doc.metadata['image']:\n",
    "            b64_images.append(doc.metadata['image'])\n",
    "        else:\n",
    "            texts.append(doc.page_content)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "retriever_multi_modal = vector_store_multi_modal.as_retriever(search_type=\"similarity\")\n",
    "\n",
    "# RAG pipeline\n",
    "chain_multimodal_rag = (\n",
    "    {\n",
    "        \"context\": retriever_multi_modal | RunnableLambda(get_image_text),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(img_prompt_func)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "chain_multimodal_rag.invoke(\"Which component are part of RLHF shown in green dash lines?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2003f7-4bad-452e-a490-af56d47af7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
